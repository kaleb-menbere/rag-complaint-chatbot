{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Task 2: Text Chunking, Embedding, and Vector Store Indexing\n",
    "# \n",
    "# ## Objective\n",
    "# Convert the cleaned text narratives into a format suitable for efficient semantic search.\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import hashlib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create necessary directories\n",
    "Path(\"data/processed\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"vector_store\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# %%\n",
    "# Load the cleaned dataset from Task 1\n",
    "print(\"Loading cleaned dataset...\")\n",
    "df = pd.read_csv('data/processed/filtered_complaints.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# %%\n",
    "# Display basic information\n",
    "print(\"Product category distribution:\")\n",
    "category_dist = df['product_category'].value_counts()\n",
    "display(category_dist)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "category_dist.plot(kind='bar')\n",
    "plt.title('Complaint Distribution by Product Category')\n",
    "plt.xlabel('Product Category')\n",
    "plt.ylabel('Number of Complaints')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Stratified Sampling\n",
    "print(\"Creating stratified sample...\")\n",
    "\n",
    "# Calculate sample size (10,000-15,000)\n",
    "total_samples = min(15000, len(df))  # Cap at 15,000\n",
    "print(f\"Target sample size: {total_samples}\")\n",
    "\n",
    "# Get unique categories\n",
    "categories = df['product_category'].unique()\n",
    "print(f\"Unique categories: {categories}\")\n",
    "\n",
    "# Calculate proportion of each category\n",
    "category_proportions = df['product_category'].value_counts(normalize=True)\n",
    "print(\"\\nOriginal proportions:\")\n",
    "for category, prop in category_proportions.items():\n",
    "    print(f\"  {category}: {prop:.3f} ({int(prop * len(df))} samples)\")\n",
    "\n",
    "# Calculate sample size per category\n",
    "samples_per_category = {}\n",
    "for category in categories:\n",
    "    prop = category_proportions[category]\n",
    "    samples_per_category[category] = int(prop * total_samples)\n",
    "\n",
    "# Ensure total equals target\n",
    "total_allocated = sum(samples_per_category.values())\n",
    "adjustment = total_samples - total_allocated\n",
    "\n",
    "# Adjust the largest category if needed\n",
    "if adjustment != 0:\n",
    "    largest_category = category_proportions.idxmax()\n",
    "    samples_per_category[largest_category] += adjustment\n",
    "\n",
    "print(\"\\nSampling strategy:\")\n",
    "for category, count in samples_per_category.items():\n",
    "    print(f\"  {category}: {count} samples\")\n",
    "\n",
    "# %%\n",
    "# Perform stratified sampling\n",
    "sampled_dfs = []\n",
    "for category, n_samples in samples_per_category.items():\n",
    "    category_df = df[df['product_category'] == category]\n",
    "    \n",
    "    # If we need more samples than available, take all\n",
    "    n_samples = min(n_samples, len(category_df))\n",
    "    \n",
    "    if n_samples > 0:\n",
    "        sampled_category = category_df.sample(n=n_samples, random_state=42)\n",
    "        sampled_dfs.append(sampled_category)\n",
    "\n",
    "# Combine sampled data\n",
    "sampled_df = pd.concat(sampled_dfs, ignore_index=True)\n",
    "print(f\"\\nFinal sampled dataset shape: {sampled_df.shape}\")\n",
    "print(f\"Sampling rate: {len(sampled_df)/len(df)*100:.1f}%\")\n",
    "\n",
    "# Check new distribution\n",
    "print(\"\\nSampled distribution:\")\n",
    "sampled_dist = sampled_df['product_category'].value_counts(normalize=True)\n",
    "for category, prop in sampled_dist.items():\n",
    "    print(f\"  {category}: {prop:.3f} ({len(sampled_df[sampled_df['product_category'] == category])} samples)\")\n",
    "\n",
    "# %%\n",
    "# Save the sampled dataset\n",
    "sampled_path = 'data/processed/sampled_complaints.csv'\n",
    "sampled_df.to_csv(sampled_path, index=False)\n",
    "print(f\"\\nSaved sampled dataset to: {sampled_path}\")\n",
    "\n",
    "# %%\n",
    "# Text Chunking Implementation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TEXT CHUNKING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken  # For token counting\n",
    "\n",
    "# Let's analyze text lengths first\n",
    "text_lengths = sampled_df['cleaned_narrative'].apply(lambda x: len(str(x).split()))\n",
    "print(\"Text length statistics (words):\")\n",
    "print(f\"  Mean: {text_lengths.mean():.1f}\")\n",
    "print(f\"  Median: {text_lengths.median():.1f}\")\n",
    "print(f\"  Min: {text_lengths.min():.1f}\")\n",
    "print(f\"  Max: {text_lengths.max():.1f}\")\n",
    "print(f\"  95th percentile: {text_lengths.quantile(0.95):.1f}\")\n",
    "\n",
    "# %%\n",
    "# Function to count tokens (approximate)\n",
    "def count_tokens(text):\n",
    "    \"\"\"Approximate token count (1 token â‰ˆ 4 characters for English)\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "# Test different chunk sizes\n",
    "test_text = sampled_df['cleaned_narrative'].iloc[0]\n",
    "print(\"\\nTesting chunking on sample text:\")\n",
    "print(f\"Original text length: {len(test_text)} characters\")\n",
    "print(f\"Approximate tokens: {count_tokens(test_text)}\")\n",
    "\n",
    "# %%\n",
    "# Create text splitter\n",
    "# Based on the pre-built vector store specifications:\n",
    "# Chunk size: 500 characters, Overlap: 50 characters\n",
    "chunk_size = 500\n",
    "chunk_overlap = 50\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Test chunking\n",
    "test_chunks = text_splitter.split_text(test_text)\n",
    "print(f\"\\nNumber of chunks created: {len(test_chunks)}\")\n",
    "print(f\"Chunk sizes: {[len(chunk) for chunk in test_chunks]}\")\n",
    "\n",
    "# Display sample chunks\n",
    "print(\"\\nSample chunks:\")\n",
    "for i, chunk in enumerate(test_chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)\n",
    "\n",
    "# %%\n",
    "# Apply chunking to entire sampled dataset\n",
    "print(\"\\nApplying chunking to entire dataset...\")\n",
    "\n",
    "chunks = []\n",
    "metadata = []\n",
    "\n",
    "for idx, row in sampled_df.iterrows():\n",
    "    text = str(row['cleaned_narrative'])\n",
    "    \n",
    "    # Skip empty texts\n",
    "    if not text.strip():\n",
    "        continue\n",
    "    \n",
    "    # Split text into chunks\n",
    "    text_chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Create metadata for each chunk\n",
    "    for chunk_idx, chunk in enumerate(text_chunks):\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        metadata.append({\n",
    "            'complaint_id': row.get('Complaint ID', f'id_{idx}'),\n",
    "            'product_category': row['product_category'],\n",
    "            'product': row.get('Product', 'Unknown'),\n",
    "            'issue': row.get('Issue', 'Unknown'),\n",
    "            'sub_issue': row.get('Sub-issue', 'Unknown'),\n",
    "            'company': row.get('Company', 'Unknown'),\n",
    "            'state': row.get('State', 'Unknown'),\n",
    "            'date_received': row.get('Date received', 'Unknown'),\n",
    "            'chunk_index': chunk_idx,\n",
    "            'total_chunks': len(text_chunks),\n",
    "            'original_length': len(text)\n",
    "        })\n",
    "\n",
    "print(f\"Total complaints processed: {len(sampled_df)}\")\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "print(f\"Average chunks per complaint: {len(chunks)/len(sampled_df):.2f}\")\n",
    "\n",
    "# %%\n",
    "# Analyze chunk statistics\n",
    "chunk_lengths = [len(chunk) for chunk in chunks]\n",
    "print(\"\\nChunk Statistics:\")\n",
    "print(f\"Mean chunk length: {np.mean(chunk_lengths):.1f} characters\")\n",
    "print(f\"Median chunk length: {np.median(chunk_lengths):.1f} characters\")\n",
    "print(f\"Min chunk length: {np.min(chunk_lengths):.1f} characters\")\n",
    "print(f\"Max chunk length: {np.max(chunk_lengths):.1f} characters\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(chunk_lengths, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Chunk Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Chunk Lengths')\n",
    "plt.axvline(chunk_size, color='red', linestyle='--', label=f'Target: {chunk_size}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Box plot by product category\n",
    "chunk_df = pd.DataFrame({\n",
    "    'chunk': chunks,\n",
    "    'length': chunk_lengths,\n",
    "    'category': [m['product_category'] for m in metadata]\n",
    "})\n",
    "category_order = chunk_df.groupby('category')['length'].median().sort_values().index\n",
    "sns.boxplot(data=chunk_df, x='category', y='length', order=category_order)\n",
    "plt.xlabel('Product Category')\n",
    "plt.ylabel('Chunk Length')\n",
    "plt.title('Chunk Length by Product Category')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Embedding Model Selection\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EMBEDDING MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Choose embedding model\n",
    "# We'll use all-MiniLM-L6-v2 as specified in requirements\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "print(f\"Selected model: {model_name}\")\n",
    "\n",
    "# Model specifications:\n",
    "# - 384 dimensions\n",
    "# - Good balance of speed and quality\n",
    "# - Well-suited for semantic search\n",
    "# - Approximately 80MB in size\n",
    "\n",
    "print(\"\\nLoading embedding model...\")\n",
    "try:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "    # Test the model\n",
    "    test_sentences = [\"Customer complaint about credit card fees\", \n",
    "                      \"Issue with money transfer service\"]\n",
    "    test_embeddings = model.encode(test_sentences)\n",
    "    print(f\"Embedding shape: {test_embeddings.shape}\")\n",
    "    print(f\"Number of dimensions: {test_embeddings.shape[1]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"Using a smaller model as fallback...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# %%\n",
    "# Generate embeddings for chunks\n",
    "print(\"\\nGenerating embeddings for chunks...\")\n",
    "\n",
    "# Process in batches to avoid memory issues\n",
    "batch_size = 128\n",
    "embeddings = []\n",
    "\n",
    "for i in range(0, len(chunks), batch_size):\n",
    "    batch = chunks[i:i + batch_size]\n",
    "    batch_embeddings = model.encode(batch, show_progress_bar=False)\n",
    "    embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    if (i // batch_size) % 10 == 0:\n",
    "        print(f\"Processed {min(i + batch_size, len(chunks))}/{len(chunks)} chunks\")\n",
    "\n",
    "embeddings_array = np.array(embeddings)\n",
    "print(f\"\\nEmbeddings shape: {embeddings_array.shape}\")\n",
    "print(f\"Memory size: {embeddings_array.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# %%\n",
    "# Vector Store Indexing\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VECTOR STORE INDEXING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Option 1: Using ChromaDB\n",
    "try:\n",
    "    import chromadb\n",
    "    from chromadb.config import Settings\n",
    "    \n",
    "    print(\"Creating ChromaDB vector store...\")\n",
    "    \n",
    "    # Initialize ChromaDB client\n",
    "    chroma_client = chromadb.PersistentClient(path=\"vector_store/chroma_db\")\n",
    "    \n",
    "    # Create or get collection\n",
    "    collection_name = \"complaint_chunks\"\n",
    "    \n",
    "    # Check if collection exists\n",
    "    existing_collections = chroma_client.list_collections()\n",
    "    if collection_name in [col.name for col in existing_collections]:\n",
    "        print(f\"Collection '{collection_name}' already exists, recreating...\")\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "    \n",
    "    collection = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}  # Using cosine similarity\n",
    "    )\n",
    "    \n",
    "    # Prepare data for ChromaDB\n",
    "    print(\"Adding documents to ChromaDB...\")\n",
    "    \n",
    "    # Add in batches\n",
    "    batch_size = 1000\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch_end = min(i + batch_size, len(chunks))\n",
    "        \n",
    "        ids = [f\"chunk_{j}\" for j in range(i, batch_end)]\n",
    "        documents = chunks[i:batch_end]\n",
    "        metadatas = metadata[i:batch_end]\n",
    "        embeddings_batch = embeddings_array[i:batch_end].tolist()\n",
    "        \n",
    "        collection.add(\n",
    "            ids=ids,\n",
    "            documents=documents,\n",
    "            metadatas=metadatas,\n",
    "            embeddings=embeddings_batch\n",
    "        )\n",
    "        \n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"Added {batch_end}/{len(chunks)} chunks\")\n",
    "    \n",
    "    print(\"ChromaDB collection created successfully!\")\n",
    "    \n",
    "    # Test retrieval\n",
    "    test_query = \"credit card fees issue\"\n",
    "    test_query_embedding = model.encode(test_query).tolist()\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[test_query_embedding],\n",
    "        n_results=3\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTest retrieval results:\")\n",
    "    for i, doc in enumerate(results['documents'][0]):\n",
    "        print(f\"\\nResult {i+1}:\")\n",
    "        print(f\"Document: {doc[:200]}...\")\n",
    "        print(f\"Metadata: {results['metadatas'][0][i]}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"ChromaDB not available, using FAISS...\")\n",
    "\n",
    "# %%\n",
    "# Option 2: Using FAISS (if ChromaDB fails or as alternative)\n",
    "import faiss\n",
    "\n",
    "print(\"\\nCreating FAISS vector store...\")\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = embeddings_array.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product (cosine similarity)\n",
    "\n",
    "# Normalize vectors for cosine similarity\n",
    "faiss.normalize_L2(embeddings_array)\n",
    "\n",
    "# Add vectors to index\n",
    "index.add(embeddings_array)\n",
    "print(f\"FAISS index created with {index.ntotal} vectors\")\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, \"vector_store/faiss_index.bin\")\n",
    "\n",
    "# Save metadata\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "metadata_df.to_parquet(\"vector_store/chunk_metadata.parquet\", index=False)\n",
    "\n",
    "# Save chunks\n",
    "chunks_df = pd.DataFrame({\"chunk\": chunks})\n",
    "chunks_df.to_parquet(\"vector_store/chunks.parquet\", index=False)\n",
    "\n",
    "print(\"FAISS index and metadata saved successfully!\")\n",
    "\n",
    "# %%\n",
    "# Test FAISS retrieval\n",
    "print(\"\\nTesting FAISS retrieval...\")\n",
    "\n",
    "# Test query\n",
    "test_query = \"high interest rates on personal loans\"\n",
    "test_query_embedding = model.encode([test_query])\n",
    "faiss.normalize_L2(test_query_embedding)\n",
    "\n",
    "# Search\n",
    "k = 3\n",
    "distances, indices = index.search(test_query_embedding, k)\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(\"\\nTop results:\")\n",
    "for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "    print(f\"\\nResult {i+1} (similarity: {distance:.4f}):\")\n",
    "    print(f\"Chunk: {chunks[idx][:200]}...\")\n",
    "    print(f\"Metadata: {metadata[idx]['product_category']} - {metadata[idx]['issue']}\")\n",
    "\n",
    "# %%\n",
    "# Create a summary of the vector store\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"VECTOR STORE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"Total complaints in sample: {len(sampled_df)}\")\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n",
    "print(f\"Embedding dimension: {dimension}\")\n",
    "print(f\"Vector store size: {embeddings_array.nbytes / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Distribution by product category\n",
    "category_counts = {}\n",
    "for meta in metadata:\n",
    "    category = meta['product_category']\n",
    "    category_counts[category] = category_counts.get(category, 0) + 1\n",
    "\n",
    "print(\"\\nChunks by product category:\")\n",
    "for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {category}: {count} chunks ({count/len(chunks)*100:.1f}%)\")\n",
    "\n",
    "# %%\n",
    "# Save configuration\n",
    "config = {\n",
    "    \"model_name\": model_name,\n",
    "    \"chunk_size\": chunk_size,\n",
    "    \"chunk_overlap\": chunk_overlap,\n",
    "    \"total_chunks\": len(chunks),\n",
    "    \"embedding_dimension\": dimension,\n",
    "    \"sampling_strategy\": \"stratified\",\n",
    "    \"sample_size\": len(sampled_df),\n",
    "    \"total_complaints\": len(df),\n",
    "    \"vector_store_type\": \"FAISS\",\n",
    "    \"similarity_metric\": \"cosine\"\n",
    "}\n",
    "\n",
    "with open(\"vector_store/config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"\\nConfiguration saved to vector_store/config.json\")\n",
    "\n",
    "# %%\n",
    "# Create a simple retrieval function for testing\n",
    "def retrieve_similar_chunks(query, k=5):\n",
    "    \"\"\"\n",
    "    Retrieve similar chunks for a given query\n",
    "    \"\"\"\n",
    "    # Encode query\n",
    "    query_embedding = model.encode([query])\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Search\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "        results.append({\n",
    "            'rank': i + 1,\n",
    "            'similarity': float(distance),\n",
    "            'chunk': chunks[idx],\n",
    "            'metadata': metadata[idx]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the function\n",
    "print(\"\\nTesting retrieval function...\")\n",
    "test_queries = [\n",
    "    \"credit card annual fee too high\",\n",
    "    \"money transfer took too long\",\n",
    "    \"savings account interest rate low\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    results = retrieve_similar_chunks(query, k=2)\n",
    "    for result in results:\n",
    "        print(f\"  - {result['metadata']['product_category']}: {result['chunk'][:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
